{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, cv2, collections, glob\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations\n",
    "import pretrainedmodels\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import catalyst\n",
    "from catalyst.dl import utils\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "from catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\n",
    "from catalyst.contrib.nn import OneCycleLRWithWarmup\n",
    "from catalyst.utils import imread\n",
    "from catalyst.contrib.callbacks.inference_callback import InferCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = \"/home/hasan/Data Set/Bengali Digit/training/images\"\n",
    "train_file_path = \"/home/hasan/Data Set/Bengali Digit/training/files\"\n",
    "test_image_path = \"/home/hasan/Data Set/Bengali Digit/testing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = os.listdir(train_image_path)\n",
    "train_img.sort()\n",
    "train_file = glob.glob(f\"{train_file_path}/*.csv\")\n",
    "train_file.sort()\n",
    "test_img = os.listdir(test_image_path)\n",
    "test_img.sort()\n",
    "print(\"Train image folder name : \\n{}  \\n\\nTrain file name: \\n{} \\n\\nTest image folder name : \\n{}\". format(train_img, train_file, test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Train Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_file[0], usecols=['filename', 'digit'])\n",
    "df1 = pd.read_csv(train_file[1], usecols=['filename', 'digit'])\n",
    "df2 = pd.read_csv(train_file[2], usecols=['filename', 'digit'])\n",
    "df3 = pd.read_csv(train_file[3], usecols=['filename', 'digit'])\n",
    "df4 = pd.read_csv(train_file[4], usecols=['filename', 'digit'])\n",
    "new_df = df.append([df1, df2, df3, df4], ignore_index=True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\"path\": [], \"label\": []}\n",
    "\n",
    "for i, folder in enumerate(train_img):\n",
    "    path = os.path.join(train_image_path, folder)\n",
    "    file = [df, df1, df2, df3, df4]\n",
    "    \n",
    "    for img in range(len(file[i])):\n",
    "        file_one = file[i]\n",
    "        image = file_one.filename[img]\n",
    "        label = file_one.digit[img]\n",
    "        \n",
    "        full_path = os.path.join(path, image)\n",
    "            \n",
    "        #train_dict['image_name'].append(image)\n",
    "        train_dict['label'].append(label)\n",
    "        train_dict['path'].append(full_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_dict)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'path': [], 'label': []}\n",
    "\n",
    "for d in test_img:\n",
    "    path = os.path.join(test_image_path, d)\n",
    "    cls = test_img.index(d) # this is not correct class! I took it just for formality \n",
    "    for img in os.listdir(path):\n",
    "        full_path = os.path.join(path, img)\n",
    "        \n",
    "        test_dict['path'].append(full_path)\n",
    "        test_dict['label'].append(cls) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_dict)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing into Train and Valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(df.path, df.label, test_size=.2, stratify=df.label)\n",
    "print(\"x_train shape {} x_valid shape {} y_train shape {} y_valid shape {}\".format(x_train.shape, x_valid.shape, y_train.shape, y_valid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature of test dataset \n",
    "x_test = pd.Series(test_df.path)\n",
    "y_test = pd.Series(test_df.label) # don't need this one! as your wish to take or not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset:\n",
    "    def __init__(self, image, label, test_data, train_data=False):\n",
    "        self.img_path = image\n",
    "        self.img_label = label\n",
    "        self.test_data = test_data\n",
    "\n",
    "        if train_data:\n",
    "            self.aug = albumentations.Compose([\n",
    "                                albumentations.Resize(32, 32, always_apply=True),\n",
    "                                albumentations.ShiftScaleRotate(shift_limit=0.0625,\n",
    "                                                                scale_limit=0.1,\n",
    "                                                                rotate_limit=5,\n",
    "                                                                p=0.9),\n",
    "                                #albumentations.RandomBrightnessContrast(always_apply=False),\n",
    "                                albumentations.RandomRotate90(always_apply=False),\n",
    "                                albumentations.HorizontalFlip(),\n",
    "                                albumentations.VerticalFlip(),\n",
    "                                albumentations.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                                         std=(0.229, 0.224, 0.225), \n",
    "                                                         always_apply=True)              \n",
    "                                                ])\n",
    "\n",
    "        else:\n",
    "            self.aug = albumentations.Compose([\n",
    "                                albumentations.Resize(32, 32, always_apply=True),\n",
    "                                albumentations.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                                         std=(0.229, 0.224, 0.225),\n",
    "                                                         always_apply=True) \n",
    "                                ])                       \n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path) \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image1 = self.img_path[idx]\n",
    "        image2 = cv2.imread(image1)\n",
    "        image = cv2.resize(image2, (32, 32)).astype(float)\n",
    "        #image = image.reshape(128, 128, 3).astype(float)\n",
    "        #image = Image.fromarray(image).convert(\"RGB\")\n",
    "        img = self.aug(image=np.array(image))['image']\n",
    "        img1 = np.transpose(img, (2, 0, 1)).astype(np.float) \n",
    "        label = self.img_label[idx]\n",
    "        \n",
    "        img2 = torch.tensor(img1, dtype=torch.float)\n",
    "        label2 = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.test_data == 1:\n",
    "            return img2\n",
    "        \n",
    "        return img2, label2 \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Custom_Dataset(image = x_train.values, \n",
    "                           label = y_train.values, \n",
    "                           test_data=0,\n",
    "                           train_data=True\n",
    "                           )\n",
    "\n",
    "valid_set = Custom_Dataset(image = x_valid.values, \n",
    "                           label =y_valid.values, \n",
    "                           test_data = 0,\n",
    "                           train_data = False\n",
    "                           )\n",
    "\n",
    "test_set = Custom_Dataset(image = x_test.values,\n",
    "                          label = y_test.values,\n",
    "                          test_data = 1,\n",
    "                          train_data = False\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, \n",
    "                          batch_size = 32, \n",
    "                          shuffle = True,\n",
    "                          num_workers = 0)\n",
    "\n",
    "valid_loader = DataLoader(valid_set,\n",
    "                          batch_size = 32,\n",
    "                          shuffle = False,\n",
    "                          num_workers = 0)\n",
    "\n",
    "test_loader = DataLoader(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = collections.OrderedDict()\n",
    "\n",
    "loaders['train'] = train_loader\n",
    "loaders['valid'] = valid_loader \n",
    "loaders['test'] = test_loader\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet34, self).__init__()\n",
    "        self.model = pretrainedmodels.__dict__['resnet34'](pretrained='imagenet')  \n",
    "        self.l0 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, h, w = x.shape\n",
    "        x = self.model.features(x) \n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        op_layer_one = self.l0(x)\n",
    "        return op_layer_one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "num_classes = 10\n",
    "logdir = \"./logs/bengali_digit_simple_notebook_1\"\n",
    "device = utils.get_device()\n",
    "\n",
    "is_fp16_used = False\n",
    "if is_fp16_used:\n",
    "    fp16_params = dict(opt_level=\"01\") # params for fp16\n",
    "else:\n",
    "    fp16_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model, criterion, optimizer\n",
    "model = Resnet34()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0003)\n",
    "scheduler = OneCycleLRWithWarmup(\n",
    "                            optimizer,\n",
    "                            num_steps=num_epochs, \n",
    "                            lr_range=(0.005, 0.00005),\n",
    "                            warmup_steps=2,\n",
    "                            momentum_range=(0.85, 0.95)\n",
    "                            )\n",
    "\n",
    "\n",
    "# model runner\n",
    "runner = SupervisedRunner(device=device)\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    AccuracyCallback(\n",
    "                    num_classes=num_classes\n",
    "                    ),\n",
    "    #AUCCallback(num_classes=num_classes,input_key=\"targets_one_hot\", class_names=class_names),\n",
    "    #F1ScoreCallback(input_key=\"targets_one_hot\", activation=\"Softmax\"),\n",
    "    EarlyStoppingCallback(\n",
    "                         patience=2, \n",
    "                         metric=\"loss\", \n",
    "                         minimize=True, \n",
    "                         min_delta=1e-06\n",
    "                         )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders={\"train\": loaders['train'], \"valid\": loaders['valid']},\n",
    "    valid_loader = \"valid\", \n",
    "    num_epochs=num_epochs,\n",
    "    callbacks=callbacks,\n",
    "    logdir=logdir,\n",
    "    minimize_metric=False,\n",
    "    fp16=fp16_params,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Loss Graph of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_metrics(logdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_metrics(logdir=logdir, step=\"epoch\", metrics=[\"accuracy01\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/hasan/Data Set/Bengali Digit/training/images\"\n",
    "folder_name = os.listdir(root_path)\n",
    "folder_name.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_path = []\n",
    "\n",
    "for i, fold in enumerate(folder_name):\n",
    "    ALL_IMAGES = list(Path(os.path.join(root_path, fold)).glob(\"**/*.png\"))\n",
    "    ALL_IMAGES = list(filter(lambda x: not x.name.startswith(\".\"), ALL_IMAGES))\n",
    "    all_img_path = all_img_path + ALL_IMAGES\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of training images are :{}\".format(len(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(images: List[Tuple[str, np.ndarray]]):\n",
    "    _indexes = [(i, j) for i in range(2) for j in range(2)]\n",
    "    \n",
    "    f, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    for (i, j), (title, img) in zip(_indexes, images):\n",
    "        ax[i, j].imshow(img)\n",
    "        ax[i, j].set_title(title)\n",
    "    f.tight_layout()\n",
    "    \n",
    "    \n",
    "def read_random_images(paths: List[Path]) -> List[Tuple[str, np.ndarray]]:\n",
    "    data = np.random.choice(paths, size=4)\n",
    "    result = []\n",
    "    for d in data:\n",
    "        title = f\"{d.parent.name}: {d.name}\"\n",
    "        _image = imread(d)\n",
    "        result.append((title, _image))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = read_random_images(all_img_path)\n",
    "show_examples(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with Valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted result of valid dataset\n",
    "predictions = np.vstack(list(map(\n",
    "    lambda x: x[\"logits\"].cpu().numpy(), \n",
    "    runner.predict_loader(loader=loaders[\"valid\"], resume=f\"{logdir}/checkpoints/best.pth\")\n",
    ")))\n",
    "print(predictions.shape)\n",
    "\n",
    "\n",
    "# taking only one predicted result\n",
    "print(\"logits: \", predictions[0])\n",
    "\n",
    "probabilities = torch.softmax(torch.from_numpy(predictions[0]), dim=0)\n",
    "print(\"probabilities: \", probabilities)\n",
    "\n",
    "label = probabilities.argmax().item()\n",
    "print(\"Predicted label is :{}\".format(label))\n",
    "#print(f\"predicted: {class_names[label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All predicted labels\n",
    "valid_label_dict = {\"label\": []}\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    probabilities = torch.softmax(torch.from_numpy(predictions[i]), dim=0)\n",
    "    label = probabilities.argmax().item()\n",
    "    valid_label_dict['label'].append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here label columns data are incorrect \n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = collections.OrderedDict([(\"infer\", loaders[\"test\"])])\n",
    "\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=test_loader,\n",
    "    callbacks=[InferCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pro = runner.callbacks[0].predictions[\"logits\"]\n",
    "predicted_pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
